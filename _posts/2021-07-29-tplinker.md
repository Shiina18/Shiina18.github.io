---
title: "论文笔记: TPLinker 实体关系抽取"
categories: 
- Machine Learning
tags: NLP
updated: 
comments: true
mathjax: true
---

- Wang, Y., Yu, B., Zhang, Y., Liu, T., Zhu, H., & Sun, L. (2020). [Tplinker: Single-stage joint extraction of entities and relations through token pair linking](https://arxiv.org/abs/2010.13415). *arXiv preprint arXiv:2010.13415*.

有作者的 [开源 PyTorch 实现](https://github.com/131250208/TPlinker-joint-extraction).

<!-- more -->

## 主要解决的问题: 两阶段方法中训练和推理阶段的数据分布不一致

实体关系抽取传统方法先抽取实体, 再预测实体对的关系 (分类问题). 这种 **两阶段** 方法无法考虑两个子任务之间的联系. 因此引入联合 (joint) 抽取, 即同时抽取实体和关系. 此外还要解决实体重叠问题 (同一个实体出现在多个三元组中).

![](https://shiina18.github.io/assets/posts/images/20210729141153304_12040.png)

大多数现有处理实体对重叠 (EntityPairOverlap, EPO) 和单实体重叠 (SingleEntiyOverlap, SEO) 的方法可分为两类:

- 基于解码器的模型使用编码器-解码器结构, 解码器每次抽取一个词或一个元组.
- 基于分解的模型先识别候选主语实体, 然后标注宾语实体和关系.

这些方法有共同的问题: **曝光偏差** (exposure bias). 编码器-解码器在训练阶段用了真实数据的 tokens, 但推理阶段用的是模型自己生成的 tokens. 因此训练和推理阶段用的数据来自不同分布. 基于分解的模型也有类似问题.

## 方法

TPLinker 是一个 **单阶段** 方法, 将联合抽取任务转化为 **T**oken **P**air **Link**ing 问题. 

### 序列标注

给定一个句子, 两个位置 $p_1$ 和 $p_2$, 和一个关系 $r$, TPLinker 回答三个判断题:  

- $p_1$ 和 $p_2$ 是否分别为同一个实体的开始和结束位置? 紫色: entity head to entity tail (EH-to-ET)
- $p_1$ 和 $p_2$ 是否分别为关系是 $r$ 的两个实体的开始位置? 红色: subject head to object head (SH-to-OH)
- $p_1$ 和 $p_2$ 是否分别为关系是 $r$ 的两个实体的结束位置? 蓝色: subject tail to object tail (ST-to-OT)

在矩阵中用 1 存储上述标签, 不同标签用不同矩阵. 

![](https://shiina18.github.io/assets/posts/images/20210729152522371_6090.png)

由于实体的尾部必然在头部后面, 所以对第一类标签, 矩阵左下三角都是 0, 可以舍去以节约内存. 而对另外两类标签, 宾语可能出现在主语前, 所以不能直接舍去矩阵左下角. 办法是把左下角的 1 翻到右上角, 存储为 2, 再把左下角舍去. 最后把矩阵摊平便于计算. (作者这里兜了圈子, 直接说在所有 token 对上标注就完事了, 1 是正向, 2 是反向.)

![](https://shiina18.github.io/assets/posts/images/20210729153212959_8133.png)

这样如上图可以自然地处理单实体重叠问题 (De Blasio 出现在两个三元组中). 为了解决实体对重叠问题, 需要对每一个关系都做序列标注. 如果有 $R$ 个关系, 就要做 $2R + 1$ 次序列标注 (第一类标签不涉及关系, 可以共用), 每个子任务生成长度为 $n(n+1)/2$ 的序列 (所有可能的 token pair 数), 其中 $n$ 为输入序列长度.

![](https://shiina18.github.io/assets/posts/images/20210729154149790_605.png)

> It seems that our tagging scheme is extremely inefficient because the length of the tagging sequence increases in a square number with increasing sentence length. Fortunately, our experiment reveals that by utilizing a lightweight tagging model on the top of the encoder, TPLinker can achieve competitive running efficiency compared with the state-of-the-art model, since the encoder is shared by all taggers (see Figure 3) and only needs to generate $n$ token representations for once.

### 模型

对一个长度为 $n$ 的句子 $[w_1, \dots, w_n]$, 先用 encoder 把每个 token $w_i$ 映射为向量 $c_i$, 然后用通常方法 (拼接输入向量, 线性变换后接激活函数) 生成 token pair $(w_i, w_j)$ 的表示

$$
h_{i, j} = \operatorname{tanh}\left(W_h [c_i, c_j] + b_h\right), \quad j\ge i,
$$

其中 $W_h$ 和 $b_h$ 是训练参数.

之后是分类问题的通常方法 (线性变换后接 softmax 得到概率分布预测) 生成这个 token pair 的 link label (0, 1, 2) 的概率预测

$$
Y_{i,j} \sim \operatorname{softmax}(W_o h_{i, j} + b_o).
$$

上述操作对每个子任务都做一遍. 损失函数为对数极大似然

$$
L = - \frac1n \sum_{t\in T} \sum_{i=1}^n \sum_{j\ge i}^n  \log \mathbb P(Y^{(t)}_{i, j} = l^{(t)}),
$$

其中 $T$ 是序列标注全体子任务, $l^{(t)}$ 是对应子任务的真实 link label (0, 1, 2).

预测

$$
\operatorname{link}(w_i, w_j, t) = \operatorname{arg\,max}_l \mathbb P(Y^{(t)}_{i, j} = l).
$$

## Further reading

- 苏剑林. (2020, Jan 3). [用 bert4keras 做三元组抽取](https://kexue.fm/archives/7161) [Blog post]. *科学空间*.
- 苏剑林. (2021, May 1). [GlobalPointer: 用统一的方式处理嵌套和非嵌套 NER](https://kexue.fm/archives/8373) [Blog post]. *科学空间*.

还没读: [陈丹琦用 pipeline 方式刷新关系抽取 SOTA](https://zhuanlan.zhihu.com/p/274938894)