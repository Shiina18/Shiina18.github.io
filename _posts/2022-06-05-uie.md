---
title: "UIE: Universal Information Extraction"
categories: 
- Machine Learning
tags: NLP
updated: 2022-06-06
comments: true
mathjax: true
---

- Lu, Y., Liu, Q., Dai, D., Xiao, X., Lin, H., Han, X., ... & Wu, H. (2022). [Unified Structure Generation for Universal Information Extraction](https://arxiv.org/abs/2203.12277). *arXiv preprint arXiv:2203.12277*.

UIE 的想法是把各类信息抽取任务统一为以「prompt + 文本」为输入, 用不同 prompt 代表不同任务, 输出生成答案的形式. 这个想法并不新颖, 早在 2019 年谷歌就发过一篇 T5 模型 (目前 3.5k+ 引用), 以 transformer 的 encoder-decoder 为架构, 用多种任务预训练模型, 把任务视为用不同的 prompt 做 seq2seq 的生成任务.

- Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2019). [Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/abs/1910.10683). *arXiv preprint arXiv:1910.10683*. [[Code](https://github.com/google-research/text-to-text-transfer-transformer)]

下面依次简单讲一讲 prompt, T5, 以及 UIE.

<!-- more -->

## Prompt

Prompt 意思是提示符 (比如 shell 中的 `$`). 想法来自让下游任务的形式更贴近预训练任务的形式, 提升小样本学习的效果. 目前用于分类和生成任务.

主要有两类:

- 用前缀或者后缀 (prefix prompt), 比较适合自回归 LM 或者生成式. 比如输入 "translate English to German: That is good.", 输出德语翻译. 
- 完形填空 (cloze prompt), 比较适合 masked language model. 比如 "[CLS]我很累,感觉很[MASK][SEP]", 输出 [MASK] 对应的字符.

问题是 prompt 需要自己设计, 有时看似差不多的 prompt 产生的结果可能大相径庭. 也有把离散 prompt 换成连续的做法, 但这更是玄学之上再套一层玄学.


- 李rumor. (2021). [Prompt范式的缘起｜Pattern-Exploiting Training](https://zhuanlan.zhihu.com/p/396971490)
- 李rumor. (2021). [Prompt范式第二阶段｜Prefix-tuning、P-tuning、Prompt-tuning](https://zhuanlan.zhihu.com/p/400790006)
- 马杀鸡三明治. (2022). [prompt到底行不行](https://zhuanlan.zhihu.com/p/474169723)

## T5

![](https://shiina18.github.io/assets/posts/images/47113116247567.png)

不多讲.

<details><summary><b>其中 T5 的文本相似度任务是当成分类任务做的</b><font color="deepskyblue"> (Show more &raquo;)</font></summary>
<p>We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.</p>
<blockquote>
<p>We were able to straightforwardly cast all of the tasks we considered into a text-to-text format with the exception of STS-B, which is a regression task where the goal is to predict a similarity score between 1 and 5. We found that most of these scores were annotated in increments of 0.2, so we simply rounded any score to the nearest increment of 0.2 and converted the result to a literal string representation of the number (e.g. the floating-point value 2.57 would be mapped to the string “2.6”). At test time, if the model outputs a string corresponding to a number between 1 and 5, we convert it to a floating-point value; otherwise, we treat the model’s prediction as incorrect. This effectively recasts the STS-B regression problem as a 21-class classification problem.</p>
</blockquote></details>

- 苏剑林. (2020). [那个屠榜的 T5 模型, 现在可以在中文上玩玩了](https://zhuanlan.zhihu.com/p/302380842)

### 预训练

One needs a dataset that is not only high quality and diverse, but also massive. To satisfy these requirements, we developed the Colossal Clean Crawled Corpus (C4), a cleaned version of Common Crawl (多但低质量) that is *two orders of magnitude larger than Wikipedia* (少但高质量).

![](https://shiina18.github.io/assets/posts/images/42961423220647.png)

<details><summary><b>The full details of the investigation can be found in the paper, including experiments on:</b><font color="deepskyblue"> (Show more &raquo;)</font></summary>
<ul>
<li><em>model architectures</em>, where we found that encoder-decoder models generally outperformed "decoder-only" language models;  </li>
<li><em>pre-training objectives</em>, where we confirmed that fill-in-the-blank-style denoising objectives (where the model is trained to recover missing words in the input) worked best and that the most important factor was the computational cost;  </li>
<li><em>unlabeled datasets</em>, where we showed that training on in-domain data can be beneficial but that pre-training on smaller datasets can lead to detrimental overfitting;  </li>
<li><em>training strategies</em>, where we found that multitask learning could be close to competitive with a pre-train-then-fine-tune approach but requires carefully choosing how often the model is trained on each task;  </li>
<li>and <em>scale</em>, where we compare scaling up the model size, the training time, and the number of ensembled models to determine how to make the best use of fixed compute power.</li>
</ul></details>

参考

- Google Research. (2020). [Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)

## UIE

![](https://shiina18.github.io/assets/posts/images/294383416245069.png)

UIE 做的事情就是新提了一系列 prompt 形式, 预训练模型. 涉及的任务包括命名实体识别, 关系抽取, 事件抽取, 情感倾向分类 (文本分类) 等. 他把抽取任务分为两个要素: spotting, 定位实体; associating, 关联实体. 模型的输入构造如下

```
[spot]...[spot]...[asso]...[asso]...[text]原文
``` 

文中将这种 prompt 称为 structural schema instructor (SSI), 这个前缀可以表示成树状形式; 把输出称为 structured extraction language (SEL).

以关系抽取为例, schema 如下

```
{
  '竞赛名称': [
    '主办方',
    '承办方'
  ],
  '其他字段'
}
```

其中 "竞赛名称" 是 spot 相当于主语字段, asso 包括 '主办方', '承办方', 相当于谓语, 而 asso 对应的值则是宾语. 其中 asso 关联到 spot 下形成树状结构 (代码也是这么实现的).

输出形式如下

```
(
    (
        竞赛名称: XX大赛 
            (主办方: A协会)
            (承办方: B协会)    
    )
    (其他字段: blahblah)
)
```

### 预训练任务

有三种.

- Text-to-structure. 对齐 Wikidata 和英文 Wikipedia 构建语料 (SSI, text, SEL). 训练模型整体.
- Masked language model. 用英文 Wikipedia 原文构建语料 (None, corrupted_source_text, corrupted_target_text). 训练语义.
- Structure generation. 从 ConceptNet 和 Wikidata 收集结构化数据构建语料 SEL, 只训练 decoder (自回归生成). 训练生成格式.

总 loss 为上述三个任务 loss (交叉熵) 相加.

### 使用相关

**标注技巧** 以中文预训练模型为例. 因为用的字段会放进 prompt 里, 所以字段名需要是中文 ([issue#2251](https://github.com/PaddlePaddle/PaddleNLP/issues/2251)), 并且和原文相似 (比如原文出现过的词).

## 读代码

官方 PyTorch 实现见 [这里](https://github.com/universal-ie/UIE), 是基于 Google T5 (huggingface 版) 的生成式模型, 提供了中文预训练模型 uie-char-small (chinese), Paddle 版的见 [DuUIE](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/information_extraction/DuUIE). 另外 PaddleNLP 实现了基于 ERINE 3.0 和指针的中文 [UIE](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/uie), 并集成到了 taskflow 接口中, 其中 uie-tiny 用的是 6 层的 ernie-3.0-medium-zh.

下面贴的代码只提出必要部分并有少量改写.

### 原版

```python
# inference.py#L36-L62

class HuggingfacePredictor:
    def __init__(self, ...):
        self._model = (
            transformers.T5ForConditionalGeneration.from_pretrained(model_path)
        )
     
    def predict(self, text):
        text = [self._ssi + x for x in text]
        inputs = self._tokenizer(text, ...)
        result = self._model.generate(...)
        return self._tokenizer.batch_decode(result, ...)
```

### PaddleNLP 2.3.0

![](https://shiina18.github.io/assets/posts/images/306691022248750.png)

和原版不同, 他不是生成式, 而是用指针得出实体的首尾 index.

```python
# model_zoo/uie/model.py

class UIE(ErniePretrainedModel):
    def __init__(self, encoding_model):
        super(UIE, self).__init__()
        self.encoder = encoding_model
        hidden_size = self.encoder.config["hidden_size"]
        self.linear_start = nn.Linear(hidden_size, 1)
        self.linear_end = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, input_ids, token_type_ids, pos_ids, att_mask):
        sequence_output, pooled_output = self.encoder(
            input_ids=input_ids,
            token_type_ids=token_type_ids,
            position_ids=pos_ids,
            attention_mask=att_mask
        )
        start_logits = self.linear_start(sequence_output)
        start_logits = paddle.squeeze(start_logits, -1)
        start_prob = self.sigmoid(start_logits)
        end_logits = self.linear_end(sequence_output)
        end_logits = paddle.squeeze(end_logits, -1)
        end_prob = self.sigmoid(end_logits)
        return start_prob, end_prob
```

他在模型外面接了两个全连接层, 分别输出每个 token 是实体开头和实体结尾的概率.

他实现的解码方式是, 考虑概率超过阈值的 index, 以实体结尾指针开始往前找到最近的实体开头指针, 绑定这两个指针对应的实体. 可见现在的方式无法处理嵌套实体.

从他的实现看, 最后模型输出的实体附带的概率是头指针和尾指针概率的乘积.

## 杂项

- [issue#2: 原版 UIE 的预训练代码也会放出](https://github.com/universal-ie/UIE/issues/2)
- [T5 应用的极简例子 text2sql](https://juejin.cn/post/7069225910427189256)
- [issue#2047: 部署](https://github.com/PaddlePaddle/PaddleNLP/issues/2047)
- [issue#2318: GPU 预测加速](https://github.com/PaddlePaddle/PaddleNLP/issues/2318)

### 分句小脚本

处理引号内标点等问题.

```python
# taskflow/utils.py#L105-L114

def cut_chinese_sent(para):
    """
    Cut the Chinese sentences more precisely.
    reference https://blog.csdn.net/blmoistawinde/article/details/82379256
    """
    para = re.sub(r'([。！？\?])([^”’])', r'\1\n\2', para)
    para = re.sub(r'(\.{6})([^”’])', r'\1\n\2', para)
    para = re.sub(r'(\…{2})([^”’])', r'\1\n\2', para)
    para = re.sub(r'([。！？\?][”’])([^，。！？\?])', r'\1\n\2', para)
    para = para.rstrip()
    return para.split("\n")
```

### 用 Pinferencia 搭建 demo

是从 [这里](https://blog.csdn.net/weixin_41778656/article/details/124416058) 看到的. 好处是真的很便利, 不过当然没有任何优化.

[Pinferencia](https://github.com/underneathall/pinferencia) 是国人开发的尚不成熟的用于快速搭建服务的 python 库. 做 demo 不错. 里面用到的 uvicorn 语法可以参考 [这里](https://www.uvicorn.org/#command-line-options).
