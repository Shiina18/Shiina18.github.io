---
title: "Spark 简要"
categories: Tech
updated: 
comments: true
mathjax: false
---

Apache Spark is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching and optimized query execution for fast queries against data of any size. Simply put, Spark is a **fast and general engine for large-scale data processing**. 快的对照物是 MapReduce, 原因在于 Spark 在内存上运行, 避免了后者串联 jobs 时频繁的磁盘 I/O 开销 (每对 map 和 reduce 之后都要进行一次). 通用的意思是它也支持了 SQL, 流处理, 机器学习, 图等一系列应用, 形成生态.

<!-- more -->

## Resilient Distributed Dataset (RDD)

RDD in Apache Spark is an immutable collection of objects which computes on the different node of the cluster. **Resilient**: fault-tolerant with the help of RDD lineage graph (DAG) and so able to recompute missing or damaged partitions due to node failures.

Spark RDD can also be **cached** and **manually partitioned**. Caching is beneficial when we use RDD several times. And manual partitioning is important to correctly balance partitions. Spark keeps **persistent** RDDs in memory by default, but it can spill them to disk if there is not enough RAM.

- [Spark RDD - Introduction, Features & Operations of RDD - DataFlair](https://data-flair.training/blogs/spark-rdd-tutorial/)

### RDD operations

RDDs support two types of operations: **transformations**, which create a new dataset from an existing one, and **actions**, which return a value to the driver program after running a computation on the dataset. For example, `map` is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, `reduce` is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program.

All transformations in Spark are **lazy**, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently.

Certain operations within Spark trigger an event known as the **shuffle**. The shuffle is Spark's mechanism for re-distributing data so that it's grouped differently across partitions. This typically involves copying data across executors and machines, making the shuffle a complex and costly operation. See examples [here](https://spark.apache.org/docs/latest/rdd-programming-guide.html#background).

## Shared variables

Normally, when a function passed to a Spark operation (such as `map` or `reduce`) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. These variables are copied to each machine, and no updates to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of *shared variables* for two common usage patterns: broadcast variables and accumulators.

**Broadcast variables** allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.

**Accumulators** are variables that are only "added" to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums.

参考

- [RDD Programming Guide - Spark 3.2.1 Documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html)
- 小怪兽. (2021). [如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？](https://www.zhihu.com/question/27974418/answer/1862026844)
- 秦续业. (2020). [既然 Python 的库能对大数据进行分析，那为何还要用 Hadoop 和 Spark?](https://www.zhihu.com/question/353387716/answer/1209209960)

其他

- [Flink 简介](https://www.zhihu.com/question/448385595/answer/1943363373)
- [Flink vs Spark](https://www.zhihu.com/question/437569406/answer/1769368747)