---
title: "SimCSE: simple contrastive sentence embedding"
categories: 
- Machine Learning
tags: NLP
updated: 2022-07-12
comments: true
mathjax: true
---

参考

- Gao, T., Yao, X., & Chen, D. (2021). Simcse: Simple contrastive learning of sentence embeddings. *arXiv preprint arXiv:2104.08821*.
- Wang, T., & Isola, P. (2020, November). Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In *International Conference on Machine Learning* (pp. 9929-9939). PMLR.

<!-- more -->

## 前置知识

对比学习输出 embedding, 目标是自动构造相似和相异的实例 (自监督), 使得相似实例的 embedding 接近, 而相异的远离. (和 w2v 相似)

BERT 的句向量不好用 (因为 CLS 用于 NSP 任务, 对词向量做操作也不好), SimCSE 利用 BERT 以对比学习的方式训练句向量.

**对比学习的两个指标**

Alignment 让相似实例 embedding 接近,

$$
\min_f\mathbb E_{(x, x^+) \sim p_{\text{positive}}} \Vert f(x) - f(x^+) \Vert^2.
$$

Uniformity 让 embedding 均匀分布在超球面上, 让它们更 informative (熵最大). 如果只看上一个指标, 那模型把所有东西映射为一个常数即可.

$$
\min_f \log \mathbb E_{x, y \overset{\text{i.i.d.}}{\sim} p_{\text{data}}} {\mathrm{e}^{-2\Vert f(x) - f(y) \Vert^2}},
$$

可以证明 $f(x)$ 均匀分布是使其最小化的唯一解 (如果可以取到, 具体略, 这是 well known 的结论).

## 核心想法

训练集 $\\{(x_i, x_i^+)\\}$, 其中 $x_i$ 和  $x_i^+$ 是语义相关的正例对, embedding 分别为 $h_i = f(x_i)$ 和 $h_i^+$, 其中 $f$ 是模型. 第 $i$ 个 pair 的损失函数为

$$
\ell_i = \log\frac{\mathrm{e}^{\operatorname{sim}(h_i, h_i^+) / \tau}}
{\sum_{j=1}^N\mathrm{e}^{\operatorname{sim}(h_i, h_j^+) / \tau}},
$$

其中 $N$ 是 batch size, $\tau$ 是温度参数, $\operatorname{sim}$ 通常是余弦相似度. 

无监督 SimCSE 构造正例的方法是, 让同一个句子 forward pass 时用不同的 dropout mask, 得到 $h_i$, $h_i^+$, 而负例则是同一个 batch 内的其他句子. 有监督略.


**注意点:** 对比学习 batch size 要足够大, 参考 [这里](https://www.zhihu.com/question/483524293/answer/2327482420) 或 [这里](https://spaces.ac.cn/archives/8586).

**实践**

- 张俊林. (2021). [利用 Contrastive Learning 对抗数据噪声：对比学习在微博场景的实践](https://zhuanlan.zhihu.com/p/370782081)
- 肖洋. (2022). [对比学习在有赞的应用](https://mp.weixin.qq.com/s/w2jSKVf5m2e64cYtZG0IDw). 有赞技术.

**TODO**

- 更一般的对比学习: [对比学习 (Contrastive Learning) 在 CV 与 NLP 领域中的研究进展](https://mp.weixin.qq.com/s/UlV-6wBZSGIH7y2uWaAAtQ)
- 度量学习
