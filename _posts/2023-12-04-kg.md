---
title: "读论文: Direct Fact Retrieval from Knowledge Graphs without Entity Linking"
categories: 
- Machine Learning
tags: NLP
updated: 
comments: true
mathjax: false
---

Baek, J., Aji, A. F., Lehmann, J., & Hwang, S. J. (2023). [Direct Fact Retrieval from Knowledge Graphs without Entity Linking](https://arxiv.org/abs/2305.12416). *arXiv preprint arXiv:2305.12416*.

简单粗暴的召回 + 排序. 流程是标准的, 粗暴点 ("创新点") 在于直接输入句子与知识库中的东西算相似度. 两句话讲完.

<!-- more -->

![](https://shiina18.github.io/assets/posts/images/399773210249686.png)

![图片来自 sentence-transformers 文档](https://shiina18.github.io/assets/posts/images/109281210231260.png "图片来自 sentence-transformers 文档")

召回 **Direct Knowledge Graph Retrieval.** 跳过实体识别+实体链接的步骤, 直接学习 (对比学习) 输入句子和三元组 (也用句子表示) 的相似度. 其中句子和三元组的 embedidng 是分别独立完成的 (三元组 embedding 线下先算好存着, 双塔, 见上图左边). 论文用了 distilbert (66M params).

排序 (精排) **Reranking for Accurate Fact Retrieval.** 先根据相似度召回若干候选三元组, 然后把句子和三元组同时 (拼接在一起, 单塔 point-wise, 见上图右边) 输入另外一个模型得到相似度. 论文用了 MiniLM (22M params).

![](https://shiina18.github.io/assets/posts/images/569534814249671.png)

看论文的效果不加排序的效果不好. 没有开源代码.

For supervised learning experiments, we train all models for 30 epochs, with a batch size of 512 for question answering and 32 for dialogue, and a learning rate of 2e-5.
